name: LLM Benchmark CI

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  benchmark:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ollama pytest pytest-html

      - name: Install Ollama
        run: |
          # Use official install script instead of manual tgz download
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version
          # Start Ollama server in background
          ollama serve & 
          sleep 10 # Wait for initialization

      - name: Pull LLM Models
        run: |
          ollama pull llama3.2:1b
          ollama pull qwen2.5:0.5b

      - name: Run Benchmark Tests
        run: |
          # Run all test files in the directory and combine them into one report
          pytest -v -s --html=benchmark_report.html --self-contained-html test_local_benchmark.py test_llm_judge.py
        continue-on-error: true

      - name: Upload Test Report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-html-report
          path: benchmark_report.html
